{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AfifaMasood/AfifaMasood/blob/main/Copy_of_transfer_learning_with_movinet_shoplifting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOgDUDMAG6mn"
      },
      "source": [
        "##### Copyright 2022 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "B3PsBDmGG_W8"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifkGYxdCHIof"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/video/transfer_learning_with_movinet\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/video/transfer_learning_with_movinet.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/video/transfer_learning_with_movinet.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/video/transfer_learning_with_movinet.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWxDDkRwLVMC"
      },
      "source": [
        "# Transfer learning for video classification with MoViNet\n",
        "\n",
        "MoViNets (Mobile Video Networks) provide a family of efficient video classification models, supporting inference on streaming video. In this tutorial, you will use a pre-trained MoViNet model to classify videos, specifically for an action recognition task, from the [UCF101 dataset](https://www.crcv.ucf.edu/data/UCF101.php). A pre-trained model is a saved network that was previously trained on a larger dataset. You can find more details about MoViNets in the [MoViNets: Mobile Video Networks for Efficient Video Recognition](https://arxiv.org/abs/2103.11511) paper by Kondratyuk, D. et al. (2021). In this tutorial, you will:\n",
        "\n",
        "* Learn how to download a pre-trained MoViNet model\n",
        "* Create a new model using a pre-trained model with a new classifier by freezing the convolutional base of the MoViNet model\n",
        "* Replace the classifier head with the number of labels of a new dataset\n",
        "* Perform transfer learning on the [UCF101 dataset](https://www.crcv.ucf.edu/data/UCF101.php)\n",
        "\n",
        "The model downloaded in this tutorial is from [official/projects/movinet](https://github.com/tensorflow/models/tree/master/official/projects/movinet). This repository contains a collection of MoViNet models that TF Hub uses in the TensorFlow 2 SavedModel format.\n",
        "\n",
        "This transfer learning tutorial is the third part in a series of TensorFlow video tutorials. Here are the other three tutorials:\n",
        "\n",
        "- [Load video data](https://www.tensorflow.org/tutorials/load_data/video): This tutorial explains much of the code used in this document; in particular, how to preprocess and load data through the `FrameGenerator` class is explained in more detail.\n",
        "- [Build a 3D CNN model for video classification](https://www.tensorflow.org/tutorials/video/video_classification). Note that this tutorial uses a (2+1)D CNN that decomposes the spatial and temporal aspects of 3D data; if you are using volumetric data such as an MRI scan, consider using a 3D CNN instead of a (2+1)D CNN.\n",
        "- [MoViNet for streaming action recognition](https://www.tensorflow.org/hub/tutorials/movinet): Get familiar with the MoViNet models that are available on TF Hub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GidiisyXwK--"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Begin by installing and importing some necessary libraries, including:\n",
        "[remotezip](https://github.com/gtsystem/python-remotezip) to inspect the contents of a ZIP file, [tqdm](https://github.com/tqdm/tqdm) to use a progress bar, [OpenCV](https://opencv.org/) to process video files (ensure that `opencv-python` and `opencv-python-headless` are the same version), and TensorFlow models ([`tf-models-official`](https://github.com/tensorflow/models/tree/master/official)) to download the pre-trained MoViNet model. The TensorFlow models package are a collection of models that use TensorFlow’s high-level APIs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nubWhqYdwEXD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89d3e5b3-9c10-422f-8293-8fbfd6a3efba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (1.26.4)\n",
            "Collecting opencv-python-headless\n",
            "  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python-headless) (1.26.4)\n",
            "Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: opencv-python-headless\n",
            "Successfully installed opencv-python-headless-4.11.0.86\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Collecting tf-models-official\n",
            "  Downloading tf_models_official-2.18.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting Cython (from tf-models-official)\n",
            "  Downloading Cython-3.0.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from tf-models-official) (11.1.0)\n",
            "Collecting ai-edge-litert>=1.0.1 (from tf-models-official)\n",
            "  Downloading ai_edge_litert-1.0.1-cp311-cp311-manylinux_2_17_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.11/dist-packages (from tf-models-official) (0.5.0)\n",
            "Collecting google-api-python-client>=1.6.7 (from tf-models-official)\n",
            "  Downloading google_api_python_client-2.160.0-py2.py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.11/dist-packages (from tf-models-official) (4.2.1)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.11/dist-packages (from tf-models-official) (1.6.17)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from tf-models-official) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from tf-models-official) (1.26.4)\n",
            "Collecting oauth2client (from tf-models-official)\n",
            "  Downloading oauth2client-4.1.3-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (from tf-models-official) (4.11.0.86)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from tf-models-official) (2.2.2)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.11/dist-packages (from tf-models-official) (5.9.5)\n",
            "Collecting py-cpuinfo>=3.3.0 (from tf-models-official)\n",
            "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
            "Collecting pycocotools (from tf-models-official)\n",
            "  Downloading pycocotools-2.0.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from tf-models-official) (6.0.2)\n",
            "Collecting sacrebleu (from tf-models-official)\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from tf-models-official) (1.13.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from tf-models-official) (0.2.0)\n",
            "Collecting seqeval (from tf-models-official)\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from tf-models-official) (1.17.0)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.11/dist-packages (from tf-models-official) (4.9.7)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from tf-models-official) (0.16.1)\n",
            "Collecting tensorflow-model-optimization>=0.4.1 (from tf-models-official)\n",
            "  Downloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl.metadata (904 bytes)\n",
            "Collecting tensorflow-text~=2.18.0 (from tf-models-official)\n",
            "  Downloading tensorflow_text-2.18.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting tensorflow~=2.18.0 (from tf-models-official)\n",
            "  Downloading tensorflow-2.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tf-keras>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from tf-models-official) (2.17.0)\n",
            "Requirement already satisfied: tf-slim>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tf-models-official) (1.1.0)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/lib/python3/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.20.2)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (2.27.0)\n",
            "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client>=1.6.7->tf-models-official)\n",
            "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (2.24.1)\n",
            "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client>=1.6.7->tf-models-official)\n",
            "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.11/dist-packages (from kaggle>=1.3.9->tf-models-official) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from kaggle>=1.3.9->tf-models-official) (2.9.0.post0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle>=1.3.9->tf-models-official) (2.32.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle>=1.3.9->tf-models-official) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from kaggle>=1.3.9->tf-models-official) (2.3.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle>=1.3.9->tf-models-official) (6.2.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.22.0->tf-models-official) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.22.0->tf-models-official) (2025.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.18.0->tf-models-official) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.18.0->tf-models-official) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.18.0->tf-models-official) (25.1.24)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.18.0->tf-models-official) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.18.0->tf-models-official) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.18.0->tf-models-official) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.18.0->tf-models-official) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.18.0->tf-models-official) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.18.0->tf-models-official) (4.25.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.18.0->tf-models-official) (75.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.18.0->tf-models-official) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.18.0->tf-models-official) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.18.0->tf-models-official) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.18.0->tf-models-official) (1.70.0)\n",
            "Collecting tensorboard<2.19,>=2.18 (from tensorflow~=2.18.0->tf-models-official)\n",
            "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.18.0->tf-models-official) (3.5.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.18.0->tf-models-official) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.18.0->tf-models-official) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.18.0->tf-models-official) (0.37.1)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official) (0.1.8)\n",
            "INFO: pip is looking at multiple versions of tf-keras to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tf-keras>=2.16.0 (from tf-models-official)\n",
            "  Downloading tf_keras-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->tf-models-official) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->tf-models-official) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->tf-models-official) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->tf-models-official) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->tf-models-official) (3.2.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.11/dist-packages (from oauth2client->tf-models-official) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.11/dist-packages (from oauth2client->tf-models-official) (0.4.1)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from oauth2client->tf-models-official) (4.9)\n",
            "Collecting portalocker (from sacrebleu->tf-models-official)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu->tf-models-official) (2024.11.6)\n",
            "Collecting tabulate>=0.8.9 (from sacrebleu->tf-models-official)\n",
            "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
            "Collecting colorama (from sacrebleu->tf-models-official)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting lxml (from sacrebleu->tf-models-official)\n",
            "  Downloading lxml-5.3.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.11/dist-packages (from seqeval->tf-models-official) (1.6.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->tf-models-official) (8.1.8)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->tf-models-official) (2.3)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->tf-models-official) (19.0.0)\n",
            "Requirement already satisfied: simple-parsing in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->tf-models-official) (0.1.7)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->tf-models-official) (1.16.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->tf-models-official) (0.10.2)\n",
            "Requirement already satisfied: array-record>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->tf-models-official) (0.6.0)\n",
            "Requirement already satisfied: etils>=1.9.1 in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->tf-models-official) (1.11.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow~=2.18.0->tf-models-official) (0.45.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->tf-models-official) (2024.12.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->tf-models-official) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->tf-models-official) (3.21.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official) (1.66.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official) (1.26.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client>=1.6.7->tf-models-official) (5.5.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow~=2.18.0->tf-models-official) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow~=2.18.0->tf-models-official) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow~=2.18.0->tf-models-official) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle>=1.3.9->tf-models-official) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle>=1.3.9->tf-models-official) (3.10)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (3.5.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow~=2.18.0->tf-models-official) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow~=2.18.0->tf-models-official) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow~=2.18.0->tf-models-official) (3.1.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach->kaggle>=1.3.9->tf-models-official) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.11/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official) (1.3)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.11/dist-packages (from simple-parsing->tensorflow-datasets->tf-models-official) (0.16)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow~=2.18.0->tf-models-official) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow~=2.18.0->tf-models-official) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow~=2.18.0->tf-models-official) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow~=2.18.0->tf-models-official) (0.1.2)\n",
            "Downloading tf_models_official-2.18.0-py2.py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ai_edge_litert-1.0.1-cp311-cp311-manylinux_2_17_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_api_python_client-2.160.0-py2.py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
            "Downloading tensorflow-2.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.4/615.4 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_text-2.18.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tf_keras-2.18.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Cython-3.0.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycocotools-2.0.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (458 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m458.7/458.7 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
            "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
            "Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading lxml-5.3.0-cp311-cp311-manylinux_2_28_x86_64.whl (5.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=1531fe4177de0ec18d2a98d328e5835d4642d58011feb27d513d75cd038d9983\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/92/f0/243288f899c2eacdfa8c5f9aede4c71a9bad0ee26a01dc5ead\n",
            "Successfully built seqeval\n",
            "Installing collected packages: py-cpuinfo, uritemplate, tensorflow-model-optimization, tabulate, portalocker, lxml, Cython, colorama, ai-edge-litert, tensorboard, sacrebleu, oauth2client, seqeval, pycocotools, google-auth-httplib2, tensorflow, google-api-python-client, tf-keras, tensorflow-text, tf-models-official\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.1\n",
            "    Uninstalling tensorboard-2.17.1:\n",
            "      Successfully uninstalled tensorboard-2.17.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.1\n",
            "    Uninstalling tensorflow-2.17.1:\n",
            "      Successfully uninstalled tensorflow-2.17.1\n",
            "  Attempting uninstall: tf-keras\n",
            "    Found existing installation: tf_keras 2.17.0\n",
            "    Uninstalling tf_keras-2.17.0:\n",
            "      Successfully uninstalled tf_keras-2.17.0\n",
            "  Attempting uninstall: tensorflow-text\n",
            "    Found existing installation: tensorflow-text 2.17.0\n",
            "    Uninstalling tensorflow-text-2.17.0:\n",
            "      Successfully uninstalled tensorflow-text-2.17.0\n",
            "Successfully installed Cython-3.0.11 ai-edge-litert-1.0.1 colorama-0.4.6 google-api-python-client-2.160.0 google-auth-httplib2-0.2.0 lxml-5.3.0 oauth2client-4.1.3 portalocker-3.1.1 py-cpuinfo-9.0.0 pycocotools-2.0.8 sacrebleu-2.5.1 seqeval-1.2.2 tabulate-0.9.0 tensorboard-2.18.0 tensorflow-2.18.0 tensorflow-model-optimization-0.8.0 tensorflow-text-2.18.1 tf-keras-2.18.0 tf-models-official-2.18.0 uritemplate-4.1.1\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.1.24)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install opencv-python # Install the latest version of opencv-python\n",
        "!pip install opencv-python-headless # Install the latest version of opencv-python-headless\n",
        "!pip install  tqdm tf-models-official\n",
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QImPsudoK9JI"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "import random\n",
        "import pathlib\n",
        "import itertools\n",
        "import collections\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "\n",
        "# Import the MoViNet model from TensorFlow Models (tf-models-official) for the MoViNet model\n",
        "from official.projects.movinet.modeling import movinet\n",
        "from official.projects.movinet.modeling import movinet_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2w3H4dfOPfnm"
      },
      "source": [
        "## Load data\n",
        "\n",
        "The hidden cell below defines helper functions to download a slice of data from the UCF-101 dataset, and load it into a `tf.data.Dataset`. The [Loading video data tutorial](https://www.tensorflow.org/tutorials/load_data/video) provides a detailed walkthrough of this code.\n",
        "\n",
        "The `FrameGenerator` class at the end of the hidden block is the most important utility here. It creates an iterable object that can feed data into the TensorFlow data pipeline. Specifically, this class contains a Python generator that loads the video frames along with its encoded label. The generator (`__call__`) function yields the frame array produced by `frames_from_video_file` and a one-hot encoded vector of the label associated with the set of frames.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwEhJ13_PSy6"
      },
      "outputs": [],
      "source": [
        "class FrameGenerator:\n",
        "    def __init__(self, path, n_frames, training=False):\n",
        "        \"\"\"Initializes FrameGenerator with path, number of frames, and training mode.\"\"\"\n",
        "        self.path = path\n",
        "        self.n_frames = n_frames\n",
        "        self.training = training\n",
        "        self.class_names = ['Shoplifting', 'Normal']\n",
        "        self.class_ids_for_name = {'Normal': 0, 'Shoplifting': 1}\n",
        "\n",
        "    def get_files_and_class_names(self):\n",
        "        # Collect both .mp4 and .avi files\n",
        "        video_paths = list(self.path.glob('*/*.mp4')) + list(self.path.glob('*/*.avi'))\n",
        "        classes = [p.parent.name for p in video_paths]\n",
        "        return video_paths, classes\n",
        "\n",
        "    def __call__(self):\n",
        "        video_paths, classes = self.get_files_and_class_names()\n",
        "        pairs = list(zip(video_paths, classes))\n",
        "\n",
        "        if self.training:\n",
        "            random.shuffle(pairs)\n",
        "\n",
        "        for path, name in pairs:\n",
        "            video_frames = frames_from_video_file(path, self.n_frames)\n",
        "            label = self.class_ids_for_name[name]\n",
        "            yield video_frames, label\n",
        "\n",
        "def frames_from_video_file(video_path, n_frames, output_size=(256, 256), frame_step=15):\n",
        "    \"\"\"\n",
        "    Extracts frames from a video file.\n",
        "\n",
        "    Args:\n",
        "        video_path: Path to the video file.\n",
        "        n_frames: Number of frames to extract.\n",
        "        output_size: The size to which each frame should be resized.\n",
        "        frame_step: Number of frames to skip between each extracted frame.\n",
        "\n",
        "    Returns:\n",
        "        A NumPy array of frames with shape (n_frames, height, width, channels).\n",
        "    \"\"\"\n",
        "    result = []\n",
        "    src = cv2.VideoCapture(str(video_path))\n",
        "    video_length = int(src.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    need_length = 1 + (n_frames - 1) * frame_step\n",
        "\n",
        "    # Determine the start point for frame extraction\n",
        "    if need_length > video_length:\n",
        "        start = 0\n",
        "    else:\n",
        "        max_start = video_length - need_length\n",
        "        start = random.randint(0, max_start + 1)\n",
        "\n",
        "    src.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
        "    ret, frame = src.read()\n",
        "    result.append(format_frames(frame, output_size))\n",
        "\n",
        "    # Extract frames by stepping through the video\n",
        "    for _ in range(n_frames - 1):\n",
        "        for _ in range(frame_step):\n",
        "            ret, frame = src.read()\n",
        "        if ret:\n",
        "            frame = format_frames(frame, output_size)\n",
        "            result.append(frame)\n",
        "        else:\n",
        "            result.append(np.zeros_like(result[0]))  # Fill with empty frames if video ends\n",
        "\n",
        "    src.release()\n",
        "    result = np.array(result)[..., [2, 1, 0]]  # Convert BGR to RGB\n",
        "\n",
        "    return result\n",
        "\n",
        "def format_frames(frame, output_size):\n",
        "    frame = tf.image.convert_image_dtype(frame, tf.float32)\n",
        "    frame = tf.image.resize_with_pad(frame, *output_size)\n",
        "    return frame\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYYShfhMx9DW"
      },
      "source": [
        "Create the training and test datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-twTu3_Bx-iJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc46a4ff-c719-45dc-8c83-383b1bd89ea5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "batch_size = 8\n",
        "num_frames = 8\n",
        "\n",
        "output_signature = (tf.TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32),\n",
        "                    tf.TensorSpec(shape=(), dtype=tf.int16))\n",
        "\n",
        "# Update to your Google Drive dataset paths\n",
        "train_path = pathlib.Path('/content/drive/MyDrive/split_dataset_fyp2/train')\n",
        "val_path = pathlib.Path('/content/drive/MyDrive/split_dataset_fyp2/val')\n",
        "test_path = pathlib.Path('/content/drive/MyDrive/split_dataset_fyp2/test')\n",
        "\n",
        "# Create train, validation, and test datasets\n",
        "train_ds = tf.data.Dataset.from_generator(\n",
        "    FrameGenerator(train_path, num_frames, training=True),\n",
        "    output_signature=output_signature\n",
        ").batch(batch_size)\n",
        "\n",
        "val_ds = tf.data.Dataset.from_generator(\n",
        "    FrameGenerator(val_path, num_frames),\n",
        "    output_signature=output_signature\n",
        ").batch(batch_size)\n",
        "\n",
        "test_ds = tf.data.Dataset.from_generator(\n",
        "    FrameGenerator(test_path, num_frames),\n",
        "    output_signature=output_signature\n",
        ").batch(batch_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7stgmuBCGQT"
      },
      "source": [
        "The labels generated here represent the encoding of the classes. For instance, 'ApplyEyeMakeup' is mapped to the integer Take a look at the labels of the training data to ensure that the dataset has been sufficiently shuffled."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Train Dataset: {train_ds}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmC_VgMHtQfz",
        "outputId": "6ce30575-91cf-4976-c3c5-0074245bd034"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset: <_BatchDataset element_spec=(TensorSpec(shape=(None, None, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int16, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9L2-toXCOQq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32af2fca-0be9-4fec-df8e-7bdc5aa60431"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: (8, 8, 256, 256, 3)\n",
            "Label: (8,)\n",
            "tf.Tensor([1 1 0 1 0 0 0 1], shape=(8,), dtype=int16)\n"
          ]
        }
      ],
      "source": [
        "for frames, labels in train_ds.take(1):\n",
        "  print(f\"Shape: {frames.shape}\")\n",
        "  print(f\"Label: {labels.shape}\")\n",
        "  print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "frame_count = 0\n",
        "for frames, labels in train_ds:\n",
        "    frame_count += frames.shape[1]  # frames.shape[1] gives number of frames per video\n",
        "print(f\"Total frames processed: {frame_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-eMpEQGNz1j",
        "outputId": "6799454d-86e6-4c2d-8f7b-34b98b90e69d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total frames processed: 184\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZ3qwZnpfy9c"
      },
      "source": [
        "Take a look at the shape of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxbhPqXGvc_F"
      },
      "source": [
        "## What are MoViNets?\n",
        "\n",
        "As mentioned previously, [MoViNets](https://arxiv.org/abs/2103.11511) are video classification models used for streaming video or online inference in tasks, such as action recognition. Consider using MoViNets to classify your video data for action recognition.\n",
        "\n",
        "A 2D frame based classifier is efficient and simple to run over whole videos, or streaming one frame at a time. Because they can't take temporal context into account they have limited accuracy and may give inconsistent outputs from frame to frame.\n",
        "\n",
        "A simple 3D CNN uses bidirectional temporal context which can increase accuracy and temporal consistency. These networks may require more resources and because they look into the future they can't be used for streaming data.\n",
        "\n",
        "![Standard convolution](https://www.tensorflow.org/images/tutorials/video/standard_convolution.png)\n",
        "\n",
        "The MoViNet architecture uses 3D convolutions that are \"causal\" along the time axis (like `layers.Conv1D` with `padding=\"causal\"`). This gives some of the advantages of both approaches, mainly it allow for efficient streaming.\n",
        "\n",
        "![Causal convolution](https://www.tensorflow.org/images/tutorials/video/causal_convolution.png)\n",
        "\n",
        "Causal convolution ensures that the output at time *t* is computed using only inputs up to time *t*. To demonstrate how this can make streaming more efficient, start with a simpler example you may be familiar with: an RNN. The RNN passes state forward through time:\n",
        "\n",
        "![RNN model](https://www.tensorflow.org/images/tutorials/video/rnn_comparison.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMvDkgfFZC6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35bf5eab-0a8c-4baf-cf15-0cd90b21f35d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of inputs: (1, 10, 8)\n",
            "Shape of result: (1, 10, 4)\n",
            "Shape of state: (1, 4)\n"
          ]
        }
      ],
      "source": [
        "gru = layers.GRU(units=4, return_sequences=True, return_state=True)\n",
        "\n",
        "inputs = tf.random.normal(shape=[1, 10, 8]) # (batch, sequence, channels)\n",
        "\n",
        "result, state = gru(inputs) # Run it all at once\n",
        "print(\"Shape of inputs:\", inputs.shape)\n",
        "print(\"Shape of result:\", result.shape)\n",
        "print(\"Shape of state:\", state.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7xyb5C4bTs7"
      },
      "source": [
        "By setting the RNN's `return_sequences=True` argument you ask it to return the state at the end of the computation. This allows you to pause and then continue where you left off, to get exactly the same result:\n",
        "\n",
        "![States passing in RNNs](https://www.tensorflow.org/images/tutorials/video/rnn_state_passing.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bI8FOPRRXXPa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c38f8255-969f-4807-d18a-3a74bc1971f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "first_half, state = gru(inputs[:, :5, :])   # run the first half, and capture the state # Now `state` shape is (1, 4)\n",
        "second_half, _ = gru(inputs[:,5:, :], initial_state=state)  # Use the state to continue where you left off.\n",
        "\n",
        "print(np.allclose(result[:, :5,:], first_half))\n",
        "print(np.allclose(result[:, 5:,:], second_half))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KM3MArumY_Qk"
      },
      "source": [
        "Causal convolutions can be used the same way, if handled with care. This technique was used in the [Fast Wavenet Generation Algorithm](https://arxiv.org/abs/1611.09482) by Le Paine et al. In the [MoVinet paper](https://arxiv.org/abs/2103.11511), the `state` is referred to as the \"Stream Buffer\".\n",
        "\n",
        "![States passed in causal convolution](https://www.tensorflow.org/images/tutorials/video/causal_conv_states.png)\n",
        "\n",
        "By passing this little bit of state forward, you can avoid recalculating the whole receptive field that shown above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UsxiPs8yA2e"
      },
      "source": [
        "## Download a pre-trained MoViNet model\n",
        "\n",
        "In this section, you will:\n",
        "\n",
        "1. You can create a MoViNet model using the open source code provided in [`official/projects/movinet`](https://github.com/tensorflow/models/tree/master/official/projects/movinet) from TensorFlow models.\n",
        "2. Load the pretrained weights.\n",
        "3. Freeze the convolutional base, or all other layers except the final classifier head, to speed up fine-tuning.\n",
        "\n",
        "To build the model, you can start with the `a0` configuration because it is the fastest to train when benchmarked against other models. Check out the [available MoViNet models on TensorFlow Model Garden](https://github.com/tensorflow/models/blob/master/official/projects/movinet/configs/movinet.py) to find what might work for your use case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhSCM6cee05F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61f76e2a-6003-4fe6-8900-da188b19c8cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "movinet_a0_base/\n",
            "movinet_a0_base/checkpoint\n",
            "movinet_a0_base/ckpt-1.data-00000-of-00001\n",
            "movinet_a0_base/ckpt-1.index\n"
          ]
        }
      ],
      "source": [
        "model_id = 'a0'\n",
        "resolution = 224\n",
        "\n",
        "# Clear any previous session\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Load the backbone with pre-trained weights\n",
        "backbone = movinet.Movinet(model_id=model_id)\n",
        "backbone.trainable = False\n",
        "\n",
        "# Set num_classes=600 initially to match pre-trained weights, and then redefine for fine-tuning\n",
        "model = movinet_model.MovinetClassifier(backbone=backbone, num_classes=600)\n",
        "model.build([None, None, None, None, 3])\n",
        "\n",
        "# Download and extract pre-trained weights\n",
        "!wget https://storage.googleapis.com/tf_model_garden/vision/movinet/movinet_a0_base.tar.gz -O movinet_a0_base.tar.gz -q\n",
        "!tar -xvf movinet_a0_base.tar.gz\n",
        "\n",
        "# Load the weights from the checkpoint\n",
        "checkpoint_dir = f'movinet_{model_id}_base'\n",
        "checkpoint_path = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "checkpoint = tf.train.Checkpoint(model=model)\n",
        "status = checkpoint.restore(checkpoint_path)\n",
        "status.assert_existing_objects_matched()\n",
        "\n",
        "# Redefine the model for binary classification (shoplifting vs. Normal) after loading weights\n",
        "model = movinet_model.MovinetClassifier(backbone=backbone, num_classes=2)\n",
        "model.build([None, None, None, resolution, 3])  # Now with your target resolution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BW23HVNtCXff"
      },
      "source": [
        "To build a classifier, create a function that takes the backbone and the number of classes in a dataset. The `build_classifier` function will take the backbone and the number of classes in a dataset to build the classifier. In this case, the new classifier will take a `num_classes` outputs (10 classes for this subset of UCF101)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cfAelbU5Gi3"
      },
      "outputs": [],
      "source": [
        "def build_classifier(batch_size, num_frames, resolution, backbone, num_classes):\n",
        "  \"\"\"Builds a classifier on top of a backbone model.\"\"\"\n",
        "  model = movinet_model.MovinetClassifier(\n",
        "      backbone=backbone,\n",
        "      num_classes=num_classes)\n",
        "  model.build([batch_size, num_frames, resolution, resolution, 3])\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HWSk-u7oPUZ"
      },
      "outputs": [],
      "source": [
        "model = build_classifier(batch_size, num_frames, resolution, backbone, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhbX7qdTN8lc"
      },
      "source": [
        "For this tutorial, choose the `tf.keras.optimizers.Adam` optimizer and the `tf.keras.losses.SparseCategoricalCrossentropy` loss function. Use the metrics argument to the view the accuracy of the model performance at every step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVqBLrn1tBsd"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# Clear the session to avoid any conflicts\n",
        "K.clear_session()\n",
        "\n",
        "num_epochs = 10\n",
        "loss_obj = SparseCategoricalCrossentropy(from_logits=True)\n",
        "# Instead of directly creating an optimizer object, pass the optimizer as a string\n",
        "optimizer = 'adam' # 'Adam'  # Use string identifier for Adam optimizer\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=loss_obj, optimizer=optimizer, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VflEr_t6CuQu"
      },
      "source": [
        "Train the model. After two epochs, observe a low loss with high accuracy for both the training and test sets."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If dataset is finite, you can count the steps per epoch\n",
        "steps_per_epoch = sum(1 for _ in train_ds)\n",
        "print(\"Steps per epoch:\", steps_per_epoch)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGUsMD7A5_of",
        "outputId": "7ee1bb77-97fa-42ca-cfac-5985cbd5990e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steps per epoch: 23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If dataset is finite, you can count the steps per epoch\n",
        "steps_per_epoch_val_ds = sum(1 for _ in val_ds)\n",
        "print(\"Steps per epoch:\", steps_per_epoch)\n"
      ],
      "metadata": {
        "id": "z9n1RavA7m4w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "507798b7-5739-428a-ce83-9536f4a14bda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steps per epoch: 23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZeiYzI0tqQG",
        "outputId": "f605dc40-3209-44bb-b730-41a1a4bbb1c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "23/23 [==============================] - 659s 29s/step - loss: 0.4032 - accuracy: 0.8470 - val_loss: 0.4723 - val_accuracy: 0.7949\n",
            "Epoch 2/10\n",
            "23/23 [==============================] - 657s 29s/step - loss: 0.2680 - accuracy: 0.8907 - val_loss: 0.3811 - val_accuracy: 0.8205\n",
            "Epoch 3/10\n",
            "23/23 [==============================] - 655s 29s/step - loss: 0.2157 - accuracy: 0.9290 - val_loss: 0.4211 - val_accuracy: 0.8205\n",
            "Epoch 4/10\n",
            "23/23 [==============================] - 656s 29s/step - loss: 0.1847 - accuracy: 0.9126 - val_loss: 0.3882 - val_accuracy: 0.8462\n",
            "Epoch 5/10\n",
            "23/23 [==============================] - 655s 28s/step - loss: 0.1316 - accuracy: 0.9563 - val_loss: 0.4507 - val_accuracy: 0.8462\n",
            "Epoch 6/10\n",
            "23/23 [==============================] - 654s 28s/step - loss: 0.0765 - accuracy: 0.9727 - val_loss: 0.5790 - val_accuracy: 0.8462\n",
            "Epoch 7/10\n",
            "23/23 [==============================] - 655s 28s/step - loss: 0.0771 - accuracy: 0.9781 - val_loss: 0.4941 - val_accuracy: 0.8205\n",
            "Epoch 8/10\n",
            "23/23 [==============================] - 655s 28s/step - loss: 0.0738 - accuracy: 0.9727 - val_loss: 0.4887 - val_accuracy: 0.8205\n",
            "Epoch 9/10\n",
            "23/23 [==============================] - 654s 28s/step - loss: 0.1048 - accuracy: 0.9672 - val_loss: 0.5901 - val_accuracy: 0.7949\n",
            "Epoch 10/10\n",
            "23/23 [==============================] - 653s 28s/step - loss: 0.0399 - accuracy: 0.9891 - val_loss: 0.5477 - val_accuracy: 0.8462\n"
          ]
        }
      ],
      "source": [
        "results = model.fit(train_ds,\n",
        "                    validation_data=val_ds,\n",
        "                    epochs=num_epochs,\n",
        "                    validation_freq=1,\n",
        "                    verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkLl2zF8G9W0"
      },
      "source": [
        "## Evaluate the model\n",
        "\n",
        "The model achieved high accuracy on the training dataset. Next, use Keras `Model.evaluate` to evaluate it on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqgbzOiKuxxT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4599d88c-c7be-4945-88c1-60e9a677b8f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 117s 22s/step - loss: 0.4353 - accuracy: 0.8250\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': 0.43532514572143555, 'accuracy': 0.824999988079071}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "model.evaluate(test_ds, return_dict=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkFst2gsHBwD"
      },
      "source": [
        "To visualize model performance further, use a [confusion matrix](https://www.tensorflow.org/api_docs/python/tf/math/confusion_matrix). The confusion matrix allows you to assess the performance of the classification model beyond accuracy. To build the confusion matrix for this multi-class classification problem, get the actual values in the test set and the predicted values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hssSdW9XHF_j"
      },
      "outputs": [],
      "source": [
        "def get_actual_predicted_labels(dataset):\n",
        "  \"\"\"\n",
        "    Create a list of actual ground truth values and the predictions from the model.\n",
        "\n",
        "    Args:\n",
        "      dataset: An iterable data structure, such as a TensorFlow Dataset, with features and labels.\n",
        "\n",
        "    Return:\n",
        "      Ground truth and predicted values for a particular dataset.\n",
        "  \"\"\"\n",
        "  actual = [labels for _, labels in dataset.unbatch()]\n",
        "  predicted = model.predict(dataset)\n",
        "\n",
        "  actual = tf.stack(actual, axis=0)\n",
        "  predicted = tf.concat(predicted, axis=0)\n",
        "  predicted = tf.argmax(predicted, axis=1)\n",
        "\n",
        "  return actual, predicted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TmTue6THGWO"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(actual, predicted, labels, ds_type):\n",
        "  cm = tf.math.confusion_matrix(actual, predicted)\n",
        "  ax = sns.heatmap(cm, annot=True, fmt='g')\n",
        "  sns.set(rc={'figure.figsize':(12, 12)})\n",
        "  sns.set(font_scale=1.4)\n",
        "  ax.set_title('Confusion matrix of action recognition for ' + ds_type)\n",
        "  ax.set_xlabel('Predicted Action')\n",
        "  ax.set_ylabel('Actual Action')\n",
        "  plt.xticks(rotation=90)\n",
        "  plt.yticks(rotation=0)\n",
        "  ax.xaxis.set_ticklabels(labels)\n",
        "  ax.yaxis.set_ticklabels(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RK1A1C1HH6V"
      },
      "outputs": [],
      "source": [
        "fg = FrameGenerator(train_path, num_frames, training = True)\n",
        "label_names = list(fg.class_ids_for_name.keys())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "actual, predicted = get_actual_predicted_labels(test_ds)\n",
        "plot_confusion_matrix(actual, predicted, label_names, 'test')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ok9IcQ-xqi-B",
        "outputId": "1e01fab5-b678-4f67-e984-319c7a3f1b2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 121s 23s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4AFi2e5HKEO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7e4d7de-f341-4b78-cb5c-86828903d7b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x78a0b0408bd0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x78a0b0390210>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x78a0b064c910>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x78a0b064ffd0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x78a0b015b450>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x78a0b0180990>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x78a0a8760d50>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x78a0a87a5450>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x78a0a8668c90>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x78a0a86e45d0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x78a0a8564590>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x78a0a87cbb10>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x78a0a84149d0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x78a0a8331a90>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x78a0a83ae790>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x78a0985ad3d0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <official.vision.modeling.layers.nn_layers.TemporalSoftmaxPool object at 0x78a0a8280190>, because it is not built.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "save_path = \"/content/drive/MyDrive/MOVINET_SHOPLIFTING_CPU_withouth5\"\n",
        "model.save(save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddQG9sYxa1Ib"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "Now that you have some familiarity with the MoViNet model and how to leverage various TensorFlow APIs (for example, for transfer learning), try using the code in this tutorial with your own dataset. The data does not have to be limited to video data. Volumetric data, such as MRI scans, can also be used with 3D CNNs. The NUSDAT and IMH datasets mentioned in [Brain MRI-based 3D Convolutional Neural Networks for Classification of Schizophrenia and Controls](https://arxiv.org/pdf/2003.08818.pdf) could be two such sources for MRI data.\n",
        "\n",
        "In particular, using the `FrameGenerator` class used in this tutorial and the other video data and classification tutorials will help you load data into your models.\n",
        "\n",
        "To learn more about working with video data in TensorFlow, check out the following tutorials:\n",
        "\n",
        "* [Load video data](https://www.tensorflow.org/tutorials/load_data/video)\n",
        "* [Build a 3D CNN model for video classification](https://www.tensorflow.org/tutorials/video/video_classification)\n",
        "* [MoViNet for streaming action recognition](https://www.tensorflow.org/hub/tutorials/movinet)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}